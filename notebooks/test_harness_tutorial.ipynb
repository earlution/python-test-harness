{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Test Harness Tutorial\n",
    "\n",
    "Welcome to this interactive tutorial on using the Python Test Harness! This notebook will help you learn about testing your Python code.\n",
    "\n",
    "## What is Testing?\n",
    "\n",
    "Testing is a way to check if your code works correctly. When you write a program, you want to make sure it:\n",
    "* Gives the right answers\n",
    "* Handles errors properly\n",
    "* Works for all types of input\n",
    "\n",
    "Think of testing like checking your homework before handing it in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, Let's Set Up Our Test Harness\n",
    "\n",
    "Run the cell below to get our testing tool ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from typing import Any, Callable, List, Union\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "class TestHarness:\n",
    "    def __init__(self):\n",
    "        self.total_tests = 0\n",
    "        self.passed_tests = 0\n",
    "        self.failed_tests = 0\n",
    "        \n",
    "    def run_test(self, \n",
    "                 func: Callable, \n",
    "                 input_args: List[Any], \n",
    "                 expected_output: Any,\n",
    "                 test_name: str = None) -> bool:\n",
    "        \"\"\"Run a single test case for the given function.\"\"\"\n",
    "        self.total_tests += 1\n",
    "        test_id = test_name or f\"Test #{self.total_tests}\"\n",
    "        \n",
    "        try:\n",
    "            actual_output = func(*input_args)\n",
    "            \n",
    "            if actual_output == expected_output:\n",
    "                self.passed_tests += 1\n",
    "                print(f\"✓ {test_id} PASSED\")\n",
    "                print(f\"  Input: {input_args}\")\n",
    "                print(f\"  Expected: {expected_output}\")\n",
    "                print(f\"  Actual: {actual_output}\\n\")\n",
    "                return True\n",
    "            else:\n",
    "                self.failed_tests += 1\n",
    "                print(f\"✗ {test_id} FAILED\")\n",
    "                print(f\"  Input: {input_args}\")\n",
    "                print(f\"  Expected: {expected_output}\")\n",
    "                print(f\"  Actual: {actual_output}\\n\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.failed_tests += 1\n",
    "            print(f\"✗ {test_id} FAILED - Exception raised\")\n",
    "            print(f\"  Input: {input_args}\")\n",
    "            print(f\"  Expected: {expected_output}\")\n",
    "            print(f\"  Error: {str(e)}\")\n",
    "            print(f\"  Traceback: {traceback.format_exc()}\\n\")\n",
    "            return False\n",
    "            \n",
    "    def run_test_suite(self, \n",
    "                       func: Callable, \n",
    "                       test_cases: List[tuple[List[Any], Any, Union[str, None]]]) -> None:\n",
    "        \"\"\"Run multiple test cases for the given function.\"\"\"\n",
    "        print(f\"Starting test suite at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Testing function: {func.__name__}\\n\")\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            if len(test_case) == 3:\n",
    "                input_args, expected_output, test_name = test_case\n",
    "            else:\n",
    "                input_args, expected_output = test_case\n",
    "                test_name = None\n",
    "            self.run_test(func, input_args, expected_output, test_name)\n",
    "            \n",
    "        print(\"\\nTest Suite Summary:\")\n",
    "        print(f\"Total tests: {self.total_tests}\")\n",
    "        print(f\"Passed: {self.passed_tests}\")\n",
    "        print(f\"Failed: {self.failed_tests}\")\n",
    "        print(f\"Success rate: {(self.passed_tests/self.total_tests)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Testing a Simple Function\n",
    "\n",
    "Let's start with something simple - a function that adds 5 to any number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Here's our function\n",
    "def add_five(number):\n",
    "    return number + 5\n",
    "\n",
    "# Create our test harness\n",
    "harness = TestHarness()\n",
    "\n",
    "# Create our test cases\n",
    "test_cases = [\n",
    "    ([0], 5, \"Adding five to zero\"),\n",
    "    ([10], 15, \"Adding five to ten\"),\n",
    "    ([-3], 2, \"Adding five to a negative number\")\n",
    "]\n",
    "\n",
    "# Run the tests!\n",
    "harness.run_test_suite(add_five, test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn!\n",
    "\n",
    "Now modify the function below to add a different number. Change the number 5 to something else, then update the test cases to match your new function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your modified function here\n",
    "def add_ten(number):\n",
    "    return number + 10\n",
    "\n",
    "# Create new test cases for your function\n",
    "test_cases = [\n",
    "    ([0], 10, \"Adding ten to zero\"),\n",
    "    ([5], 15, \"Adding ten to five\"),\n",
    "    ([-5], 5, \"Adding ten to negative five\")\n",
    "]\n",
    "\n",
    "# Run your tests\n",
    "harness = TestHarness()\n",
    "harness.run_test_suite(add_ten, test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Testing a Grade Calculator\n",
    "\n",
    "Let's try something more complex - a function that converts marks to grades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_grade(mark):\n",
    "    if mark >= 70:\n",
    "        return 'A'\n",
    "    elif mark >= 60:\n",
    "        return 'B'\n",
    "    elif mark >= 50:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'F'\n",
    "\n",
    "# Create test cases\n",
    "test_cases = [\n",
    "    ([75], 'A', \"Testing A grade\"),\n",
    "    ([65], 'B', \"Testing B grade\"),\n",
    "    ([55], 'C', \"Testing C grade\"),\n",
    "    ([45], 'F', \"Testing F grade\"),\n",
    "    ([70], 'A', \"Testing boundary - lowest A\"),\n",
    "    ([69], 'B', \"Testing boundary - highest B\")\n",
    "]\n",
    "\n",
    "# Run the tests\n",
    "harness = TestHarness()\n",
    "harness.run_test_suite(calculate_grade, test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Create Tests for the Temperature Converter\n",
    "\n",
    "Here's a function that converts Celsius to Fahrenheit. Can you write tests for it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def celsius_to_fahrenheit(celsius):\n",
    "    return (celsius * 1.8) + 32\n",
    "\n",
    "# Write your test cases here\n",
    "test_cases = [\n",
    "    # Add your test cases following this pattern:\n",
    "    # ([input], expected_output, \"test name\")\n",
    "]\n",
    "\n",
    "# Run your tests\n",
    "harness = TestHarness()\n",
    "harness.run_test_suite(celsius_to_fahrenheit, test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Find the Bugs!\n",
    "\n",
    "This function has some bugs. Can you find them using the test harness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for number in numbers:\n",
    "        total += number\n",
    "    return total / len(numbers)\n",
    "\n",
    "# These tests will help you find the bugs\n",
    "test_cases = [\n",
    "    ([[1, 2, 3]], 2, \"Average of 1,2,3\"),\n",
    "    ([[4, 4, 4]], 4, \"Average of same numbers\"),\n",
    "    ([[]], 0, \"Empty list\"),  # This should cause an error!\n",
    "    ([[1]], 1, \"Single number\")\n",
    "]\n",
    "\n",
    "harness = TestHarness()\n",
    "harness.run_test_suite(calculate_average, test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Challenge: Create and Test a Function\n",
    "\n",
    "Choose one of these challenges:\n",
    "1. Write a function that counts vowels in a word\n",
    "2. Write a function that finds the largest number in a list\n",
    "3. Write a function that checks if a number is even or odd\n",
    "\n",
    "Then write tests for your function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Write your function here\n",
    "\n",
    "\n",
    "# Write your test cases here\n",
    "test_cases = [\n",
    "    # Add your test cases\n",
    "]\n",
    "\n",
    "# Run your tests\n",
    "harness = TestHarness()\n",
    "harness.run_test_suite(your_function_name, test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Have We Learned?\n",
    "\n",
    "1. How to write test cases\n",
    "2. How to use a test harness\n",
    "3. Why testing is important\n",
    "4. How to find bugs using tests\n",
    "\n",
    "Remember: Good testing helps you find problems before your users do!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}